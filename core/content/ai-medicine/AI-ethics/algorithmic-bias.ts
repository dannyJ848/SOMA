import { EducationalContent } from '../../types';

export const ALGORITHMIC_BIAS: EducationalContent = {
  id: 'ai-algorithmic-bias',
  type: 'concept',
  name: 'Algorithmic Bias in Healthcare AI',
  alternateNames: ['AI Bias', 'Machine Learning Bias', 'Healthcare Algorithm Disparities', 'Computational Bias'],
  levels: {
    1: {
      level: 1,
      summary: 'Algorithmic bias happens when computer programs used in healthcare work better for some groups of people than others, which can be unfair.',
      explanation: 'Computer programs that help doctors are trained using lots of information from past patients. But if that information is not balanced, the computer might learn things that are not fair to everyone. For example, if a program was mostly trained using information from one group of people, it might not work as well for people who are different from that group. This is called algorithmic bias. Imagine a program that looks at photos to detect skin problems. If it was mostly trained on lighter skin, it might miss problems on darker skin. This is not the computer being mean on purpose - it is just learning from the information it was given. Scientists and doctors work hard to find and fix these biases so that healthcare AI can help everyone equally.',
      keyTerms: [
        { term: 'bias', definition: 'When a computer program favors some groups over others' },
        { term: 'unfairness', definition: 'Treating people differently in ways that are not right or equal' },
        { term: 'computer program', definition: 'Instructions that tell a computer what to do' },
        { term: 'training data', definition: 'Information used to teach a computer program how to work' },
        { term: 'groups', definition: 'Different types of people based on things like age, race, or where they live' },
      ],
    },
    2: {
      level: 2,
      summary: 'Algorithmic bias occurs when AI systems produce systematically different outcomes for different demographic groups, potentially perpetuating or amplifying healthcare disparities.',
      explanation: 'Healthcare AI systems learn patterns from training data to make predictions or recommendations. Algorithmic bias emerges when these systems perform differently across demographic groups defined by characteristics like race, gender, age, or socioeconomic status. Bias can enter AI systems through multiple pathways: historical data reflecting past discrimination or unequal care access; underrepresentation of certain populations in training datasets; use of proxy variables that correlate with protected characteristics; or outcome definitions that themselves reflect disparities. For example, an algorithm predicting healthcare needs that used healthcare costs as a proxy for illness severity disadvantaged Black patients, who historically received less costly care for equivalent disease burden. Detection of algorithmic bias requires testing model performance across demographic subgroups using multiple fairness metrics. Mitigation strategies include diversifying training data, adjusting algorithms to equalize performance, and carefully selecting prediction targets. Ongoing monitoring after deployment is essential, as bias can emerge or shift over time. Addressing algorithmic bias is both an ethical imperative and a practical necessity for AI systems to benefit all patients.',
      keyTerms: [
        { term: 'demographic groups', definition: 'Categories of people based on characteristics like race, gender, age, or socioeconomic status' },
        { term: 'training data', definition: 'Historical data used to develop AI models that may contain biases' },
        { term: 'healthcare disparities', definition: 'Differences in health outcomes or care quality between population groups' },
        { term: 'proxy variables', definition: 'Indirect measures that correlate with protected characteristics' },
        { term: 'fairness metrics', definition: 'Mathematical measures used to evaluate equitable AI performance' },
        { term: 'mitigation strategies', definition: 'Approaches to reduce or eliminate algorithmic bias' },
      ],
    },
    3: {
      level: 3,
      summary: 'Algorithmic bias in healthcare AI manifests through multiple mechanisms including biased training data, label noise, proxy discrimination, and feedback loops, requiring systematic detection and mitigation approaches guided by context-appropriate fairness criteria.',
      explanation: 'The taxonomy of algorithmic bias in healthcare encompasses several distinct mechanisms. Data bias arises from training datasets that fail to represent target populations due to historical access disparities, selection effects, or missing data patterns that correlate with demographic characteristics. Label bias occurs when outcome variables used for training reflect biased clinical decisions or structural inequities rather than true underlying states. Feature bias involves predictive variables that serve as proxies for protected characteristics, enabling indirect discrimination even without explicit use of demographic information. Measurement bias emerges from differential accuracy or validity of data collection across populations, such as pulse oximetry errors across skin tones or natural language processing performance across dialects. Algorithmic processing can amplify input biases through model architecture choices, optimization objectives, or threshold selection. Feedback loops may perpetuate or worsen disparities when biased predictions influence care delivery, generating similarly biased future training data. Fairness operationalization requires selecting among multiple formal definitions (demographic parity, equalized odds, predictive parity, individual fairness) that are often mathematically incompatible, demanding context-specific ethical judgment. Technical mitigation approaches span pre-processing (data augmentation, reweighting), in-processing (constrained optimization, adversarial debiasing), and post-processing (threshold adjustment, calibration) stages.',
      keyTerms: [
        { term: 'label bias', definition: 'When outcome variables used for training reflect biased clinical decisions rather than true states' },
        { term: 'proxy discrimination', definition: 'Indirect discrimination through variables correlated with protected characteristics' },
        { term: 'feedback loops', definition: 'When biased predictions influence care delivery, generating similarly biased future training data' },
        { term: 'demographic parity', definition: 'Fairness criterion requiring equal positive prediction rates across groups' },
        { term: 'equalized odds', definition: 'Fairness criterion requiring equal true positive and false positive rates across groups' },
        { term: 'predictive parity', definition: 'Fairness criterion requiring equal positive predictive value across groups' },
        { term: 'adversarial debiasing', definition: 'Technique using adversarial networks to remove demographic information from predictions' },
        { term: 'measurement bias', definition: 'Differential accuracy of data collection across populations' },
      ],
    },
    4: {
      level: 4,
      summary: 'Critical analysis of algorithmic bias in healthcare requires engagement with sociotechnical systems perspectives, structural determinants of health, mathematical fairness theory, and the political economy of AI development.',
      explanation: 'Comprehensive understanding of algorithmic bias transcends technical framing to examine healthcare AI as sociotechnical systems embedded within broader social structures. Bias in AI reflects and may reinforce structural determinants of health, including historical discrimination, economic inequality, and unequal healthcare access that shape both training data and deployment contexts. Mathematical fairness theory reveals fundamental impossibility results: except under unrealistic conditions, no classifier can simultaneously satisfy multiple intuitive fairness criteria, necessitating explicit ethical choices about which trade-offs to accept. The political economy perspective examines how AI development processes, commercial incentives, and power asymmetries influence which biases receive attention and which populations bear algorithmic harms. Intersectionality frameworks highlight how bias may compound across multiple demographic dimensions, with individuals at intersections of marginalized groups experiencing greater cumulative disadvantage. Epistemic justice concerns address whose knowledge and values shape AI development, fairness definitions, and acceptable trade-offs. Participatory approaches involving affected communities in AI design and governance offer alternatives to purely technical solutions. Critical scholars question whether bias mitigation within existing systems sufficiently addresses algorithmic injustice or whether more fundamental reforms to AI development and healthcare delivery are required. Regulatory approaches are evolving, with requirements for algorithmic impact assessments and demographic performance reporting emerging in multiple jurisdictions.',
      keyTerms: [
        { term: 'sociotechnical systems', definition: 'Framework examining AI as embedded within broader social structures' },
        { term: 'structural determinants', definition: 'Historical discrimination and systemic inequities that shape training data' },
        { term: 'impossibility theorems', definition: 'Mathematical results showing that multiple fairness criteria cannot be satisfied simultaneously' },
        { term: 'political economy', definition: 'Analysis of how commercial incentives and power influence AI development' },
        { term: 'intersectionality', definition: 'Framework examining how bias compounds across multiple demographic dimensions' },
        { term: 'epistemic justice', definition: 'Concerns about whose knowledge and values shape AI development' },
        { term: 'participatory design', definition: 'Involving affected communities in AI design and governance' },
        { term: 'algorithmic impact assessment', definition: 'Formal evaluation of potential harms before AI deployment' },
      ],
    },
    5: {
      level: 5,
      summary: 'Clinical and institutional leadership in addressing algorithmic bias requires implementation of systematic assessment, monitoring, and governance processes that translate fairness principles into operational practices while engaging with structural determinants of healthcare disparities.',
      explanation: 'Practical leadership in algorithmic fairness demands multi-level interventions across AI lifecycle stages. Pre-deployment assessment should examine training data composition, demographic representation, and potential sources of label or measurement bias; validation studies should explicitly report performance stratified by relevant demographic characteristics using multiple fairness metrics. Procurement processes should require vendor transparency about bias testing, known limitations, and monitoring capabilities, with contractual provisions for ongoing performance reporting. Implementation protocols should specify populations for whom the AI is validated, explicitly address performance variations across groups, and establish clinical workflows for appropriate human oversight of algorithmic recommendations. Monitoring systems should continuously track AI performance across demographic strata in production environments, detecting emerging biases or performance degradation requiring intervention. Governance structures should empower oversight bodies to halt or modify AI deployments when concerning disparities emerge. Patient and community engagement should inform decisions about acceptable trade-offs and ensure algorithmic development serves affected populations. Institutional commitment must extend beyond technical fixes to address structural factors underlying healthcare disparities, recognizing that AI equity requires attention to data generation processes, healthcare access, and social determinants of health. Leadership requires ongoing education as the field evolves, engagement with emerging regulatory requirements, and willingness to prioritize equity even when it conflicts with efficiency or commercial interests.',
      keyTerms: [
        { term: 'bias assessment', definition: 'Systematic evaluation of AI performance across demographic groups' },
        { term: 'stratified validation', definition: 'Testing AI systems separately across different population subgroups' },
        { term: 'procurement requirements', definition: 'Vendor transparency obligations about bias testing and limitations' },
        { term: 'continuous monitoring', definition: 'Ongoing surveillance of AI performance in production environments' },
        { term: 'governance structures', definition: 'Oversight bodies empowered to halt or modify AI deployments' },
        { term: 'community engagement', definition: 'Involving patients and communities in AI fairness decisions' },
        { term: 'structural factors', definition: 'Underlying social determinants that generate biased data' },
        { term: 'equity prioritization', definition: 'Organizational commitment to fairness even when conflicting with efficiency' },
      ],
      clinicalNotes: 'When using AI tools in clinical practice, understand their validated populations and known performance variations across demographic groups. Apply appropriate clinical judgment when AI recommendations may be less reliable for particular patients. Report observed disparities or unexpected behaviors to designated oversight bodies. Advocate for institutional investment in AI equity assessment and monitoring. Recognize that algorithmic outputs reflect historical data patterns and may not account for individual patient circumstances.',
    },
  },
  media: [],
  citations: [
    { id: 'obermeyer-2019', type: 'article', title: 'Dissecting racial bias in an algorithm used to manage the health of populations', source: 'Science. 2019;366(6464):447-453' },
    { id: 'rajkomar-2018', type: 'article', title: 'Ensuring Fairness in Machine Learning to Advance Health Equity', source: 'Ann Intern Med. 2018;169(12):866-872' },
    { id: 'chen-2021', type: 'article', title: 'Ethical Machine Learning in Healthcare', source: 'Annu Rev Biomed Data Sci. 2021;4:123-144' },
    { id: 'mehrabi-2021', type: 'article', title: 'A Survey on Bias and Fairness in Machine Learning', source: 'ACM Comput Surv. 2021;54(6):1-35' },
  ],
  crossReferences: [
    { targetId: 'ai-ethics-healthcare', targetType: 'concept', relationship: 'related', label: 'AI Ethics in Healthcare' },
    { targetId: 'ai-transparency', targetType: 'concept', relationship: 'related', label: 'AI Transparency' },
    { targetId: 'health-data-privacy', targetType: 'concept', relationship: 'related', label: 'Health Data Privacy' },
    { targetId: 'data-security', targetType: 'concept', relationship: 'related', label: 'Data Security' },
  ],
  tags: {
    systems: ['digital-health'],
    topics: ['AI-ethics', 'algorithmic-fairness', 'healthcare-disparities', 'machine-learning', 'bias'],
    keywords: ['algorithmic bias', 'fairness', 'discrimination', 'equity', 'machine learning', 'healthcare AI'],
  },
  createdAt: '2025-01-28T00:00:00.000Z',
  updatedAt: '2025-01-28T00:00:00.000Z',
  version: 1,
  status: 'published',
  contributors: ['Biological Self Content Team'],
};
